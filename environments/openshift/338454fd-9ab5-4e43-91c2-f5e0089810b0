{"deployments": [{"inputs": [{"default": "192.168.24.17 192.168.24.17 192.168.24.17 192.168.24.17 192.168.24.17 192.168.24.17", "type": "String", "name": "ping_test_ips", "value": "192.168.24.17 192.168.24.17 192.168.24.17 192.168.24.17 192.168.24.17 192.168.24.17", "description": ""}, {"default": "False", "type": "String", "name": "validate_fqdn", "value": "False", "description": ""}, {"default": "True", "type": "String", "name": "validate_ntp", "value": "True", "description": ""}, {"type": "String", "name": "deploy_server_id", "value": "aa5c6139-4a3d-4175-b35d-e0c354d8ef4b", "description": "ID of the server being deployed to"}, {"type": "String", "name": "deploy_action", "value": "CREATE", "description": "Name of the current action being deployed"}, {"type": "String", "name": "deploy_stack_id", "value": "overcloud-baremetal-ControllerAllNodesValidationDeployment-jogfmwctggw7/3da08916-1766-4c91-9e23-183ff7d3f8ff", "description": "ID of the stack this deployment belongs to"}, {"type": "String", "name": "deploy_resource_name", "value": "0", "description": "Name of this deployment resource in the stack"}, {"type": "String", "name": "deploy_signal_transport", "value": "CFN_SIGNAL", "description": "How the server should signal to heat with the deployment output values."}, {"type": "String", "name": "deploy_signal_id", "value": "http://192.168.24.1:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3Ad1529b2fc9454dcf8dd0271abab7de1d%3Astacks%2Fovercloud-baremetal-ControllerAllNodesValidationDeployment-jogfmwctggw7%2F3da08916-1766-4c91-9e23-183ff7d3f8ff%2Fresources%2F0?Timestamp=2017-05-17T19%3A23%3A45Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=94d1131b99bf4ad78da4bf603098a5fe&SignatureVersion=2&Signature=srwQjcL%2BTfWWlNxy2X0v4Q2i9XJgs5AHSjd6Ewj6WRc%3D", "description": "ID of signal to use for signaling output values"}, {"type": "String", "name": "deploy_signal_verb", "value": "POST", "description": "HTTP verb to use for signaling outputvalues"}], "group": "script", "name": "ControllerAllNodesValidationDeployment", "outputs": [], "creation_time": "2017-05-17T19:23:50Z", "options": {}, "config": "#!/bin/bash\nset -e\n\nfunction ping_retry() {\n  local IP_ADDR=$1\n  local TIMES=${2:-'10'}\n  local COUNT=0\n  local PING_CMD=ping\n  if [[ $IP_ADDR =~ \":\" ]]; then\n    PING_CMD=ping6\n  fi\n  until [ $COUNT -ge $TIMES ]; do\n    if $PING_CMD -w 300 -c 1 $IP_ADDR &> /dev/null; then\n      echo \"Ping to $IP_ADDR succeeded.\"\n      return 0\n    fi\n    echo \"Ping to $IP_ADDR failed. Retrying...\"\n    COUNT=$(($COUNT + 1))\n  done\n  return 1\n}\n\n# For each unique remote IP (specified via Heat) we check to\n# see if one of the locally configured networks matches and if so we\n# attempt a ping test the remote network IP.\nfunction ping_controller_ips() {\n  local REMOTE_IPS=$1\n  for REMOTE_IP in $(echo $REMOTE_IPS | sed -e \"s| |\\n|g\" | sort -u); do\n    if [[ $REMOTE_IP =~ \":\" ]]; then\n      networks=$(ip -6 r | grep -v default | cut -d \" \" -f 1 | grep -v \"unreachable\")\n    else\n      networks=$(ip r | grep -v default | cut -d \" \" -f 1)\n    fi\n    for LOCAL_NETWORK in $networks; do\n      in_network=$(python -c \"import ipaddr; net=ipaddr.IPNetwork('$LOCAL_NETWORK'); addr=ipaddr.IPAddress('$REMOTE_IP'); print(addr in net)\")\n      if [[ $in_network == \"True\" ]]; then\n        echo \"Trying to ping $REMOTE_IP for local network ${LOCAL_NETWORK}.\"\n        set +e\n        if ! ping_retry $REMOTE_IP; then\n          echo \"FAILURE\"\n          echo \"$REMOTE_IP is not pingable. Local Network: $LOCAL_NETWORK\" >&2\n          exit 1\n        fi\n        set -e\n        echo \"SUCCESS\"\n      fi\n    done\n  done\n}\n\n# Ping all default gateways. There should only be one\n# if using upstream t-h-t network templates but we test\n# all of them should some manual network config have\n# multiple gateways.\nfunction ping_default_gateways() {\n  DEFAULT_GW=$(ip r | grep ^default | cut -d \" \" -f 3)\n  set +e\n  for GW in $DEFAULT_GW; do\n    echo -n \"Trying to ping default gateway ${GW}...\"\n    if ! ping_retry $GW; then\n      echo \"FAILURE\"\n      echo \"$GW is not pingable.\"\n      exit 1\n    fi\n  done\n  set -e\n  echo \"SUCCESS\"\n}\n\n# Verify the FQDN from the nova/ironic deployment matches\n# FQDN in the heat templates.\nfunction fqdn_check() {\n  HOSTNAME=$(hostname)\n  SHORT_NAME=$(hostname -s)\n  FQDN_FROM_HOSTS=$(awk '$3 == \"'${SHORT_NAME}'\"{print $2}' /etc/hosts)\n  echo -n \"Checking hostname vs /etc/hosts entry...\"\n  if [[ $HOSTNAME != $FQDN_FROM_HOSTS ]]; then\n    echo \"FAILURE\"\n    echo -e \"System hostname: ${HOSTNAME}\\nEntry from /etc/hosts: ${FQDN_FROM_HOSTS}\\n\"\n    exit 1\n  fi\n  echo \"SUCCESS\"\n}\n\n# Verify at least one time source is available.\nfunction ntp_check() {\n  NTP_SERVERS=$(hiera ntp::servers nil |tr -d '[],\"')\n  if [[ \"$NTP_SERVERS\" != \"nil\" ]];then\n    echo -n \"Testing NTP...\"\n    NTP_SUCCESS=0\n    for NTP_SERVER in $NTP_SERVERS; do\n      set +e\n      NTPDATE_OUT=$(ntpdate -qud $NTP_SERVER 2>&1)\n      NTPDATE_EXIT=$?\n      set -e\n      if [[ \"$NTPDATE_EXIT\" == \"0\" ]];then\n        NTP_SUCCESS=1\n        break\n      else\n        NTPDATE_OUT_FULL=\"$NTPDATE_OUT_FULL $NTPDATE_OUT\"\n      fi\n    done\n    if  [[ \"$NTP_SUCCESS\" == \"0\" ]];then\n      echo \"FAILURE\"\n      echo \"$NTPDATE_OUT_FULL\"\n      exit 1\n    fi\n    echo \"SUCCESS\"\n  fi\n}\n\nping_controller_ips \"$ping_test_ips\"\nping_default_gateways\nif [[ $validate_fqdn == \"True\" ]];then\n  fqdn_check\nfi\nif [[ $validate_ntp == \"True\" ]];then\n  ntp_check\nfi\n", "id": "a47a6347-cf09-468f-ad98-3bfc017b15bf"}, {"inputs": [{"type": "String", "name": "bootstack_nodeid", "value": "overcloud-controller-0"}, {"type": "String", "name": "enable_load_balancer", "value": true}, {"type": "String", "name": "enable_package_upgrade", "value": "false"}, {"type": "String", "name": "deploy_server_id", "value": "aa5c6139-4a3d-4175-b35d-e0c354d8ef4b", "description": "ID of the server being deployed to"}, {"type": "String", "name": "deploy_action", "value": "CREATE", "description": "Name of the current action being deployed"}, {"type": "String", "name": "deploy_stack_id", "value": "overcloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm/d0b4efff-786e-4da2-85cf-a2becd27242a", "description": "ID of the stack this deployment belongs to"}, {"type": "String", "name": "deploy_resource_name", "value": "ControllerDeployment", "description": "Name of this deployment resource in the stack"}, {"type": "String", "name": "deploy_signal_transport", "value": "CFN_SIGNAL", "description": "How the server should signal to heat with the deployment output values."}, {"type": "String", "name": "deploy_signal_id", "value": "http://192.168.24.1:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3Ad1529b2fc9454dcf8dd0271abab7de1d%3Astacks%2Fovercloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm%2Fd0b4efff-786e-4da2-85cf-a2becd27242a%2Fresources%2FControllerDeployment?Timestamp=2017-05-17T19%3A14%3A08Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=364486593bed4d3796a09614c2dbead6&SignatureVersion=2&Signature=iJ9l5M866mTxC4VHpxv%2BCRw%2BFwU4cTp%2FEuU6lwoACLY%3D", "description": "ID of signal to use for signaling output values"}, {"type": "String", "name": "deploy_signal_verb", "value": "POST", "description": "HTTP verb to use for signaling outputvalues"}], "group": "hiera", "name": "ControllerDeployment", "outputs": [], "creation_time": "2017-05-17T19:21:29Z", "options": {}, "config": {"hierarchy": ["\"%{::uuid}\"", "heat_config_%{::deploy_config_name}", "controller_extraconfig", "extraconfig", "service_configs", "service_names", "controller", "bootstrap_node", "all_nodes", "vip_data", "\"%{::osfamily}\"", "neutron_bigswitch_data", "neutron_cisco_data", "cisco_n1kv_data", "midonet_data", "cisco_aci_data"], "datafiles": {"service_names": {"sensu::subscriptions": [], "service_names": []}, "controller": {"tripleo::packages::enable_upgrade": "false", "enable_load_balancer": true, "fqdn_ctlplane": "overcloud-controller-0.ctlplane.localdomain", "fqdn_management": "overcloud-controller-0.management.localdomain", "fqdn_internal_api": "overcloud-controller-0.internalapi.localdomain", "fqdn_storage_mgmt": "overcloud-controller-0.storagemgmt.localdomain", "tripleo::haproxy::service_certificate": null, "bootstack_nodeid": "overcloud-controller-0", "fqdn_storage": "overcloud-controller-0.storage.localdomain", "fqdn_tenant": "overcloud-controller-0.tenant.localdomain"}, "extraconfig": {}, "service_configs": {}, "controller_extraconfig": {}}, "merge_behavior": "deeper"}, "id": "c6fcb4d7-4425-44eb-ae9b-3ccba4995f2c"}, {"inputs": [{"default": "192.168.24.13  overcloud.localdomain\n192.168.24.13  overcloud.ctlplane.localdomain\n192.168.24.13  overcloud.internalapi.localdomain\n192.168.24.13  overcloud.storage.localdomain\n192.168.24.13  overcloud.storagemgmt.localdomain\n192.168.24.17 overcloud-controller-0.localdomain overcloud-controller-0\n192.168.24.17 overcloud-controller-0.external.localdomain overcloud-controller-0.external\n192.168.24.17 overcloud-controller-0.internalapi.localdomain overcloud-controller-0.internalapi\n192.168.24.17 overcloud-controller-0.storage.localdomain overcloud-controller-0.storage\n192.168.24.17 overcloud-controller-0.storagemgmt.localdomain overcloud-controller-0.storagemgmt\n192.168.24.17 overcloud-controller-0.tenant.localdomain overcloud-controller-0.tenant\n192.168.24.17 overcloud-controller-0.management.localdomain overcloud-controller-0.management\n192.168.24.17 overcloud-controller-0.ctlplane.localdomain overcloud-controller-0.ctlplane\n192.168.24.9 overcloud-controller-1.localdomain overcloud-controller-1\n192.168.24.9 overcloud-controller-1.external.localdomain overcloud-controller-1.external\n192.168.24.9 overcloud-controller-1.internalapi.localdomain overcloud-controller-1.internalapi\n192.168.24.9 overcloud-controller-1.storage.localdomain overcloud-controller-1.storage\n192.168.24.9 overcloud-controller-1.storagemgmt.localdomain overcloud-controller-1.storagemgmt\n192.168.24.9 overcloud-controller-1.tenant.localdomain overcloud-controller-1.tenant\n192.168.24.9 overcloud-controller-1.management.localdomain overcloud-controller-1.management\n192.168.24.9 overcloud-controller-1.ctlplane.localdomain overcloud-controller-1.ctlplane\n192.168.24.6 overcloud-controller-2.localdomain overcloud-controller-2\n192.168.24.6 overcloud-controller-2.external.localdomain overcloud-controller-2.external\n192.168.24.6 overcloud-controller-2.internalapi.localdomain overcloud-controller-2.internalapi\n192.168.24.6 overcloud-controller-2.storage.localdomain overcloud-controller-2.storage\n192.168.24.6 overcloud-controller-2.storagemgmt.localdomain overcloud-controller-2.storagemgmt\n192.168.24.6 overcloud-controller-2.tenant.localdomain overcloud-controller-2.tenant\n192.168.24.6 overcloud-controller-2.management.localdomain overcloud-controller-2.management\n192.168.24.6 overcloud-controller-2.ctlplane.localdomain overcloud-controller-2.ctlplane\n\n192.168.24.11 overcloud-compute-0.localdomain overcloud-compute-0\n192.168.24.11 overcloud-compute-0.external.localdomain overcloud-compute-0.external\n192.168.24.11 overcloud-compute-0.internalapi.localdomain overcloud-compute-0.internalapi\n192.168.24.11 overcloud-compute-0.storage.localdomain overcloud-compute-0.storage\n192.168.24.11 overcloud-compute-0.storagemgmt.localdomain overcloud-compute-0.storagemgmt\n192.168.24.11 overcloud-compute-0.tenant.localdomain overcloud-compute-0.tenant\n192.168.24.11 overcloud-compute-0.management.localdomain overcloud-compute-0.management\n192.168.24.11 overcloud-compute-0.ctlplane.localdomain overcloud-compute-0.ctlplane\n\n192.168.24.12 overcloud-blockstorage-0.localdomain overcloud-blockstorage-0\n192.168.24.12 overcloud-blockstorage-0.external.localdomain overcloud-blockstorage-0.external\n192.168.24.12 overcloud-blockstorage-0.internalapi.localdomain overcloud-blockstorage-0.internalapi\n192.168.24.12 overcloud-blockstorage-0.storage.localdomain overcloud-blockstorage-0.storage\n192.168.24.12 overcloud-blockstorage-0.storagemgmt.localdomain overcloud-blockstorage-0.storagemgmt\n192.168.24.12 overcloud-blockstorage-0.tenant.localdomain overcloud-blockstorage-0.tenant\n192.168.24.12 overcloud-blockstorage-0.management.localdomain overcloud-blockstorage-0.management\n192.168.24.12 overcloud-blockstorage-0.ctlplane.localdomain overcloud-blockstorage-0.ctlplane\n\n192.168.24.19 overcloud-openshift-0.localdomain overcloud-openshift-0\n192.168.24.19 overcloud-openshift-0.external.localdomain overcloud-openshift-0.external\n192.168.24.19 overcloud-openshift-0.internalapi.localdomain overcloud-openshift-0.internalapi\n192.168.24.19 overcloud-openshift-0.storage.localdomain overcloud-openshift-0.storage\n192.168.24.19 overcloud-openshift-0.storagemgmt.localdomain overcloud-openshift-0.storagemgmt\n192.168.24.19 overcloud-openshift-0.tenant.localdomain overcloud-openshift-0.tenant\n192.168.24.19 overcloud-openshift-0.management.localdomain overcloud-openshift-0.management\n192.168.24.19 overcloud-openshift-0.ctlplane.localdomain overcloud-openshift-0.ctlplane\n192.168.24.10 overcloud-openshift-1.localdomain overcloud-openshift-1\n192.168.24.10 overcloud-openshift-1.external.localdomain overcloud-openshift-1.external\n192.168.24.10 overcloud-openshift-1.internalapi.localdomain overcloud-openshift-1.internalapi\n192.168.24.10 overcloud-openshift-1.storage.localdomain overcloud-openshift-1.storage\n192.168.24.10 overcloud-openshift-1.storagemgmt.localdomain overcloud-openshift-1.storagemgmt\n192.168.24.10 overcloud-openshift-1.tenant.localdomain overcloud-openshift-1.tenant\n192.168.24.10 overcloud-openshift-1.management.localdomain overcloud-openshift-1.management\n192.168.24.10 overcloud-openshift-1.ctlplane.localdomain overcloud-openshift-1.ctlplane\n192.168.24.14 overcloud-openshift-2.localdomain overcloud-openshift-2\n192.168.24.14 overcloud-openshift-2.external.localdomain overcloud-openshift-2.external\n192.168.24.14 overcloud-openshift-2.internalapi.localdomain overcloud-openshift-2.internalapi\n192.168.24.14 overcloud-openshift-2.storage.localdomain overcloud-openshift-2.storage\n192.168.24.14 overcloud-openshift-2.storagemgmt.localdomain overcloud-openshift-2.storagemgmt\n192.168.24.14 overcloud-openshift-2.tenant.localdomain overcloud-openshift-2.tenant\n192.168.24.14 overcloud-openshift-2.management.localdomain overcloud-openshift-2.management\n192.168.24.14 overcloud-openshift-2.ctlplane.localdomain overcloud-openshift-2.ctlplane\n", "type": "String", "name": "hosts", "value": "192.168.24.13  overcloud.localdomain\n192.168.24.13  overcloud.ctlplane.localdomain\n192.168.24.13  overcloud.internalapi.localdomain\n192.168.24.13  overcloud.storage.localdomain\n192.168.24.13  overcloud.storagemgmt.localdomain\n192.168.24.17 overcloud-controller-0.localdomain overcloud-controller-0\n192.168.24.17 overcloud-controller-0.external.localdomain overcloud-controller-0.external\n192.168.24.17 overcloud-controller-0.internalapi.localdomain overcloud-controller-0.internalapi\n192.168.24.17 overcloud-controller-0.storage.localdomain overcloud-controller-0.storage\n192.168.24.17 overcloud-controller-0.storagemgmt.localdomain overcloud-controller-0.storagemgmt\n192.168.24.17 overcloud-controller-0.tenant.localdomain overcloud-controller-0.tenant\n192.168.24.17 overcloud-controller-0.management.localdomain overcloud-controller-0.management\n192.168.24.17 overcloud-controller-0.ctlplane.localdomain overcloud-controller-0.ctlplane\n192.168.24.9 overcloud-controller-1.localdomain overcloud-controller-1\n192.168.24.9 overcloud-controller-1.external.localdomain overcloud-controller-1.external\n192.168.24.9 overcloud-controller-1.internalapi.localdomain overcloud-controller-1.internalapi\n192.168.24.9 overcloud-controller-1.storage.localdomain overcloud-controller-1.storage\n192.168.24.9 overcloud-controller-1.storagemgmt.localdomain overcloud-controller-1.storagemgmt\n192.168.24.9 overcloud-controller-1.tenant.localdomain overcloud-controller-1.tenant\n192.168.24.9 overcloud-controller-1.management.localdomain overcloud-controller-1.management\n192.168.24.9 overcloud-controller-1.ctlplane.localdomain overcloud-controller-1.ctlplane\n192.168.24.6 overcloud-controller-2.localdomain overcloud-controller-2\n192.168.24.6 overcloud-controller-2.external.localdomain overcloud-controller-2.external\n192.168.24.6 overcloud-controller-2.internalapi.localdomain overcloud-controller-2.internalapi\n192.168.24.6 overcloud-controller-2.storage.localdomain overcloud-controller-2.storage\n192.168.24.6 overcloud-controller-2.storagemgmt.localdomain overcloud-controller-2.storagemgmt\n192.168.24.6 overcloud-controller-2.tenant.localdomain overcloud-controller-2.tenant\n192.168.24.6 overcloud-controller-2.management.localdomain overcloud-controller-2.management\n192.168.24.6 overcloud-controller-2.ctlplane.localdomain overcloud-controller-2.ctlplane\n\n192.168.24.11 overcloud-compute-0.localdomain overcloud-compute-0\n192.168.24.11 overcloud-compute-0.external.localdomain overcloud-compute-0.external\n192.168.24.11 overcloud-compute-0.internalapi.localdomain overcloud-compute-0.internalapi\n192.168.24.11 overcloud-compute-0.storage.localdomain overcloud-compute-0.storage\n192.168.24.11 overcloud-compute-0.storagemgmt.localdomain overcloud-compute-0.storagemgmt\n192.168.24.11 overcloud-compute-0.tenant.localdomain overcloud-compute-0.tenant\n192.168.24.11 overcloud-compute-0.management.localdomain overcloud-compute-0.management\n192.168.24.11 overcloud-compute-0.ctlplane.localdomain overcloud-compute-0.ctlplane\n\n192.168.24.12 overcloud-blockstorage-0.localdomain overcloud-blockstorage-0\n192.168.24.12 overcloud-blockstorage-0.external.localdomain overcloud-blockstorage-0.external\n192.168.24.12 overcloud-blockstorage-0.internalapi.localdomain overcloud-blockstorage-0.internalapi\n192.168.24.12 overcloud-blockstorage-0.storage.localdomain overcloud-blockstorage-0.storage\n192.168.24.12 overcloud-blockstorage-0.storagemgmt.localdomain overcloud-blockstorage-0.storagemgmt\n192.168.24.12 overcloud-blockstorage-0.tenant.localdomain overcloud-blockstorage-0.tenant\n192.168.24.12 overcloud-blockstorage-0.management.localdomain overcloud-blockstorage-0.management\n192.168.24.12 overcloud-blockstorage-0.ctlplane.localdomain overcloud-blockstorage-0.ctlplane\n\n192.168.24.19 overcloud-openshift-0.localdomain overcloud-openshift-0\n192.168.24.19 overcloud-openshift-0.external.localdomain overcloud-openshift-0.external\n192.168.24.19 overcloud-openshift-0.internalapi.localdomain overcloud-openshift-0.internalapi\n192.168.24.19 overcloud-openshift-0.storage.localdomain overcloud-openshift-0.storage\n192.168.24.19 overcloud-openshift-0.storagemgmt.localdomain overcloud-openshift-0.storagemgmt\n192.168.24.19 overcloud-openshift-0.tenant.localdomain overcloud-openshift-0.tenant\n192.168.24.19 overcloud-openshift-0.management.localdomain overcloud-openshift-0.management\n192.168.24.19 overcloud-openshift-0.ctlplane.localdomain overcloud-openshift-0.ctlplane\n192.168.24.10 overcloud-openshift-1.localdomain overcloud-openshift-1\n192.168.24.10 overcloud-openshift-1.external.localdomain overcloud-openshift-1.external\n192.168.24.10 overcloud-openshift-1.internalapi.localdomain overcloud-openshift-1.internalapi\n192.168.24.10 overcloud-openshift-1.storage.localdomain overcloud-openshift-1.storage\n192.168.24.10 overcloud-openshift-1.storagemgmt.localdomain overcloud-openshift-1.storagemgmt\n192.168.24.10 overcloud-openshift-1.tenant.localdomain overcloud-openshift-1.tenant\n192.168.24.10 overcloud-openshift-1.management.localdomain overcloud-openshift-1.management\n192.168.24.10 overcloud-openshift-1.ctlplane.localdomain overcloud-openshift-1.ctlplane\n192.168.24.14 overcloud-openshift-2.localdomain overcloud-openshift-2\n192.168.24.14 overcloud-openshift-2.external.localdomain overcloud-openshift-2.external\n192.168.24.14 overcloud-openshift-2.internalapi.localdomain overcloud-openshift-2.internalapi\n192.168.24.14 overcloud-openshift-2.storage.localdomain overcloud-openshift-2.storage\n192.168.24.14 overcloud-openshift-2.storagemgmt.localdomain overcloud-openshift-2.storagemgmt\n192.168.24.14 overcloud-openshift-2.tenant.localdomain overcloud-openshift-2.tenant\n192.168.24.14 overcloud-openshift-2.management.localdomain overcloud-openshift-2.management\n192.168.24.14 overcloud-openshift-2.ctlplane.localdomain overcloud-openshift-2.ctlplane\n", "description": ""}, {"type": "String", "name": "deploy_server_id", "value": "aa5c6139-4a3d-4175-b35d-e0c354d8ef4b", "description": "ID of the server being deployed to"}, {"type": "String", "name": "deploy_action", "value": "CREATE", "description": "Name of the current action being deployed"}, {"type": "String", "name": "deploy_stack_id", "value": "overcloud-baremetal-ControllerHostsDeployment-b4ayjz3l7a27/d55b1633-3cb5-4a57-9fc1-707bde6d87f3", "description": "ID of the stack this deployment belongs to"}, {"type": "String", "name": "deploy_resource_name", "value": "0", "description": "Name of this deployment resource in the stack"}, {"type": "String", "name": "deploy_signal_transport", "value": "CFN_SIGNAL", "description": "How the server should signal to heat with the deployment output values."}, {"type": "String", "name": "deploy_signal_id", "value": "http://192.168.24.1:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3Ad1529b2fc9454dcf8dd0271abab7de1d%3Astacks%2Fovercloud-baremetal-ControllerHostsDeployment-b4ayjz3l7a27%2Fd55b1633-3cb5-4a57-9fc1-707bde6d87f3%2Fresources%2F0?Timestamp=2017-05-17T19%3A22%3A50Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=c8a5bd41cc7c4ccba8ceab1ce26baa65&SignatureVersion=2&Signature=fwkjee2G9BOUswuQReoOCtTstaf2ZBr8WM3uU7iHgA0%3D", "description": "ID of signal to use for signaling output values"}, {"type": "String", "name": "deploy_signal_verb", "value": "POST", "description": "HTTP verb to use for signaling outputvalues"}], "group": "script", "name": "ControllerHostsDeployment", "outputs": [], "creation_time": "2017-05-17T19:23:02Z", "options": {}, "config": "#!/bin/bash\nset -eux\nset -o pipefail\n\nwrite_entries() {\n    local file=\"$1\"\n    local entries=\"$2\"\n\n    # Don't do anything if the file isn't there\n    if [ ! -f \"$file\" ]; then\n        return\n    fi\n\n    if grep -q \"^# HEAT_HOSTS_START\" \"$file\"; then\n        temp=$(mktemp)\n        (\n        sed '/^# HEAT_HOSTS_START/,$d' \"$file\"\n        echo -ne \"\\n# HEAT_HOSTS_START - Do not edit manually within this section!\\n\"\n        echo \"$entries\"\n        echo -ne \"# HEAT_HOSTS_END\\n\\n\"\n        sed '1,/^# HEAT_HOSTS_END/d' \"$file\"\n        ) > \"$temp\"\n        echo \"INFO: Updating hosts file $file, check below for changes\"\n        diff \"$file\" \"$temp\" || true\n        cat \"$temp\" > \"$file\"\n    else\n        echo -ne \"\\n# HEAT_HOSTS_START - Do not edit manually within this section!\\n\" >> \"$file\"\n        echo \"$entries\" >> \"$file\"\n        echo -ne \"# HEAT_HOSTS_END\\n\\n\" >> \"$file\"\n    fi\n\n}\n\nif [ ! -z \"$hosts\" ]; then\n    for tmpl in /etc/cloud/templates/hosts.*.tmpl ; do\n        write_entries \"$tmpl\" \"$hosts\"\n    done\n    write_entries \"/etc/hosts\" \"$hosts\"\nelse\n    echo \"No hosts in Heat, nothing written.\"\nfi\n", "id": "2f9bbf80-07f7-404e-94bc-616dd306e927"}, {"inputs": [{"default": "192.168.24.17,overcloud-controller-0.localdomain,overcloud-controller-0,192.168.24.17,overcloud-controller-0.external.localdomain,overcloud-controller-0.external,192.168.24.17,overcloud-controller-0.internalapi.localdomain,overcloud-controller-0.internalapi,192.168.24.17,overcloud-controller-0.storage.localdomain,overcloud-controller-0.storage,192.168.24.17,overcloud-controller-0.storagemgmt.localdomain,overcloud-controller-0.storagemgmt,192.168.24.17,overcloud-controller-0.tenant.localdomain,overcloud-controller-0.tenant,192.168.24.17,overcloud-controller-0.management.localdomain,overcloud-controller-0.management,192.168.24.17,overcloud-controller-0.ctlplane.localdomain,overcloud-controller-0.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBNRyr1qJpi3X+hL2NpRKNYCtq8Z4rFbs9RVD+vSxvDuhfKwI7E5vJo1KQqWvCAY4kv6GfUJnt+UdkaOKhDIZUAI= \n192.168.24.9,overcloud-controller-1.localdomain,overcloud-controller-1,192.168.24.9,overcloud-controller-1.external.localdomain,overcloud-controller-1.external,192.168.24.9,overcloud-controller-1.internalapi.localdomain,overcloud-controller-1.internalapi,192.168.24.9,overcloud-controller-1.storage.localdomain,overcloud-controller-1.storage,192.168.24.9,overcloud-controller-1.storagemgmt.localdomain,overcloud-controller-1.storagemgmt,192.168.24.9,overcloud-controller-1.tenant.localdomain,overcloud-controller-1.tenant,192.168.24.9,overcloud-controller-1.management.localdomain,overcloud-controller-1.management,192.168.24.9,overcloud-controller-1.ctlplane.localdomain,overcloud-controller-1.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOg3YngUKE0csm/N/7IY7pK2WyAfQHEZTw9H5WlKeqzCKbEAtw6GFvjs/zQZsR2X5Fl53Lg44+MgM0IigpPpZFE= \n192.168.24.6,overcloud-controller-2.localdomain,overcloud-controller-2,192.168.24.6,overcloud-controller-2.external.localdomain,overcloud-controller-2.external,192.168.24.6,overcloud-controller-2.internalapi.localdomain,overcloud-controller-2.internalapi,192.168.24.6,overcloud-controller-2.storage.localdomain,overcloud-controller-2.storage,192.168.24.6,overcloud-controller-2.storagemgmt.localdomain,overcloud-controller-2.storagemgmt,192.168.24.6,overcloud-controller-2.tenant.localdomain,overcloud-controller-2.tenant,192.168.24.6,overcloud-controller-2.management.localdomain,overcloud-controller-2.management,192.168.24.6,overcloud-controller-2.ctlplane.localdomain,overcloud-controller-2.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOeMSI2fVH2FwomzmQsua127HT0BR5IvHJCAqZsopVYDn1J2+s0cf7KyUPIM2ewDrv+hNrZ1EDvECt4E+Uzi5xg= \n192.168.24.11,overcloud-compute-0.localdomain,overcloud-compute-0,192.168.24.11,overcloud-compute-0.external.localdomain,overcloud-compute-0.external,192.168.24.11,overcloud-compute-0.internalapi.localdomain,overcloud-compute-0.internalapi,192.168.24.11,overcloud-compute-0.storage.localdomain,overcloud-compute-0.storage,192.168.24.11,overcloud-compute-0.storagemgmt.localdomain,overcloud-compute-0.storagemgmt,192.168.24.11,overcloud-compute-0.tenant.localdomain,overcloud-compute-0.tenant,192.168.24.11,overcloud-compute-0.management.localdomain,overcloud-compute-0.management,192.168.24.11,overcloud-compute-0.ctlplane.localdomain,overcloud-compute-0.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBNp8X1teTbKw3b7qNUFT9J1dghNAg7YJoxtxy3wHHgb/SjoMoeTH9VOiGHBLx9vRIDtC4GkN370mLtqxotjPD+k= \n192.168.24.12,overcloud-blockstorage-0.localdomain,overcloud-blockstorage-0,192.168.24.12,overcloud-blockstorage-0.external.localdomain,overcloud-blockstorage-0.external,192.168.24.12,overcloud-blockstorage-0.internalapi.localdomain,overcloud-blockstorage-0.internalapi,192.168.24.12,overcloud-blockstorage-0.storage.localdomain,overcloud-blockstorage-0.storage,192.168.24.12,overcloud-blockstorage-0.storagemgmt.localdomain,overcloud-blockstorage-0.storagemgmt,192.168.24.12,overcloud-blockstorage-0.tenant.localdomain,overcloud-blockstorage-0.tenant,192.168.24.12,overcloud-blockstorage-0.management.localdomain,overcloud-blockstorage-0.management,192.168.24.12,overcloud-blockstorage-0.ctlplane.localdomain,overcloud-blockstorage-0.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBDVdF/BE7a53hXNRi9NMz1eiRRoSWMtMe5UZ8+TpjU/qwU4CVWkvQlQ03MLV9L8eY+66WJfSiCoqYwCbK2dREmA= \n192.168.24.19,overcloud-openshift-0.localdomain,overcloud-openshift-0,192.168.24.19,overcloud-openshift-0.external.localdomain,overcloud-openshift-0.external,192.168.24.19,overcloud-openshift-0.internalapi.localdomain,overcloud-openshift-0.internalapi,192.168.24.19,overcloud-openshift-0.storage.localdomain,overcloud-openshift-0.storage,192.168.24.19,overcloud-openshift-0.storagemgmt.localdomain,overcloud-openshift-0.storagemgmt,192.168.24.19,overcloud-openshift-0.tenant.localdomain,overcloud-openshift-0.tenant,192.168.24.19,overcloud-openshift-0.management.localdomain,overcloud-openshift-0.management,192.168.24.19,overcloud-openshift-0.ctlplane.localdomain,overcloud-openshift-0.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBKLxrTTu0EXgF9iGm68DzA1hgRCoutNYPKT575dLCzcjBkqfA0Pi+unkMhbg0bvc0aJ1Udy7DVCj6hsaOkwoLtQ= \n192.168.24.10,overcloud-openshift-1.localdomain,overcloud-openshift-1,192.168.24.10,overcloud-openshift-1.external.localdomain,overcloud-openshift-1.external,192.168.24.10,overcloud-openshift-1.internalapi.localdomain,overcloud-openshift-1.internalapi,192.168.24.10,overcloud-openshift-1.storage.localdomain,overcloud-openshift-1.storage,192.168.24.10,overcloud-openshift-1.storagemgmt.localdomain,overcloud-openshift-1.storagemgmt,192.168.24.10,overcloud-openshift-1.tenant.localdomain,overcloud-openshift-1.tenant,192.168.24.10,overcloud-openshift-1.management.localdomain,overcloud-openshift-1.management,192.168.24.10,overcloud-openshift-1.ctlplane.localdomain,overcloud-openshift-1.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBPiEB1dhR7eivaMXYFWJdEOMHzuqUl57zcigvCDejb7s/VB7uTTwaq3ejbGrRIZHeZCwIfd5V3OIa223xgX6vLM= \n192.168.24.14,overcloud-openshift-2.localdomain,overcloud-openshift-2,192.168.24.14,overcloud-openshift-2.external.localdomain,overcloud-openshift-2.external,192.168.24.14,overcloud-openshift-2.internalapi.localdomain,overcloud-openshift-2.internalapi,192.168.24.14,overcloud-openshift-2.storage.localdomain,overcloud-openshift-2.storage,192.168.24.14,overcloud-openshift-2.storagemgmt.localdomain,overcloud-openshift-2.storagemgmt,192.168.24.14,overcloud-openshift-2.tenant.localdomain,overcloud-openshift-2.tenant,192.168.24.14,overcloud-openshift-2.management.localdomain,overcloud-openshift-2.management,192.168.24.14,overcloud-openshift-2.ctlplane.localdomain,overcloud-openshift-2.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBHtOOAGw7PDi9xzpzzSkUQrpd/tT+MQ+FnokM5sHKwt+7rxEjhXPuiYhmZLxoVxGuVXrwFaHtFTNAUuSMXy9hC0= \n", "type": "String", "name": "known_hosts", "value": "192.168.24.17,overcloud-controller-0.localdomain,overcloud-controller-0,192.168.24.17,overcloud-controller-0.external.localdomain,overcloud-controller-0.external,192.168.24.17,overcloud-controller-0.internalapi.localdomain,overcloud-controller-0.internalapi,192.168.24.17,overcloud-controller-0.storage.localdomain,overcloud-controller-0.storage,192.168.24.17,overcloud-controller-0.storagemgmt.localdomain,overcloud-controller-0.storagemgmt,192.168.24.17,overcloud-controller-0.tenant.localdomain,overcloud-controller-0.tenant,192.168.24.17,overcloud-controller-0.management.localdomain,overcloud-controller-0.management,192.168.24.17,overcloud-controller-0.ctlplane.localdomain,overcloud-controller-0.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBNRyr1qJpi3X+hL2NpRKNYCtq8Z4rFbs9RVD+vSxvDuhfKwI7E5vJo1KQqWvCAY4kv6GfUJnt+UdkaOKhDIZUAI= \n192.168.24.9,overcloud-controller-1.localdomain,overcloud-controller-1,192.168.24.9,overcloud-controller-1.external.localdomain,overcloud-controller-1.external,192.168.24.9,overcloud-controller-1.internalapi.localdomain,overcloud-controller-1.internalapi,192.168.24.9,overcloud-controller-1.storage.localdomain,overcloud-controller-1.storage,192.168.24.9,overcloud-controller-1.storagemgmt.localdomain,overcloud-controller-1.storagemgmt,192.168.24.9,overcloud-controller-1.tenant.localdomain,overcloud-controller-1.tenant,192.168.24.9,overcloud-controller-1.management.localdomain,overcloud-controller-1.management,192.168.24.9,overcloud-controller-1.ctlplane.localdomain,overcloud-controller-1.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOg3YngUKE0csm/N/7IY7pK2WyAfQHEZTw9H5WlKeqzCKbEAtw6GFvjs/zQZsR2X5Fl53Lg44+MgM0IigpPpZFE= \n192.168.24.6,overcloud-controller-2.localdomain,overcloud-controller-2,192.168.24.6,overcloud-controller-2.external.localdomain,overcloud-controller-2.external,192.168.24.6,overcloud-controller-2.internalapi.localdomain,overcloud-controller-2.internalapi,192.168.24.6,overcloud-controller-2.storage.localdomain,overcloud-controller-2.storage,192.168.24.6,overcloud-controller-2.storagemgmt.localdomain,overcloud-controller-2.storagemgmt,192.168.24.6,overcloud-controller-2.tenant.localdomain,overcloud-controller-2.tenant,192.168.24.6,overcloud-controller-2.management.localdomain,overcloud-controller-2.management,192.168.24.6,overcloud-controller-2.ctlplane.localdomain,overcloud-controller-2.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBOeMSI2fVH2FwomzmQsua127HT0BR5IvHJCAqZsopVYDn1J2+s0cf7KyUPIM2ewDrv+hNrZ1EDvECt4E+Uzi5xg= \n192.168.24.11,overcloud-compute-0.localdomain,overcloud-compute-0,192.168.24.11,overcloud-compute-0.external.localdomain,overcloud-compute-0.external,192.168.24.11,overcloud-compute-0.internalapi.localdomain,overcloud-compute-0.internalapi,192.168.24.11,overcloud-compute-0.storage.localdomain,overcloud-compute-0.storage,192.168.24.11,overcloud-compute-0.storagemgmt.localdomain,overcloud-compute-0.storagemgmt,192.168.24.11,overcloud-compute-0.tenant.localdomain,overcloud-compute-0.tenant,192.168.24.11,overcloud-compute-0.management.localdomain,overcloud-compute-0.management,192.168.24.11,overcloud-compute-0.ctlplane.localdomain,overcloud-compute-0.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBNp8X1teTbKw3b7qNUFT9J1dghNAg7YJoxtxy3wHHgb/SjoMoeTH9VOiGHBLx9vRIDtC4GkN370mLtqxotjPD+k= \n192.168.24.12,overcloud-blockstorage-0.localdomain,overcloud-blockstorage-0,192.168.24.12,overcloud-blockstorage-0.external.localdomain,overcloud-blockstorage-0.external,192.168.24.12,overcloud-blockstorage-0.internalapi.localdomain,overcloud-blockstorage-0.internalapi,192.168.24.12,overcloud-blockstorage-0.storage.localdomain,overcloud-blockstorage-0.storage,192.168.24.12,overcloud-blockstorage-0.storagemgmt.localdomain,overcloud-blockstorage-0.storagemgmt,192.168.24.12,overcloud-blockstorage-0.tenant.localdomain,overcloud-blockstorage-0.tenant,192.168.24.12,overcloud-blockstorage-0.management.localdomain,overcloud-blockstorage-0.management,192.168.24.12,overcloud-blockstorage-0.ctlplane.localdomain,overcloud-blockstorage-0.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBDVdF/BE7a53hXNRi9NMz1eiRRoSWMtMe5UZ8+TpjU/qwU4CVWkvQlQ03MLV9L8eY+66WJfSiCoqYwCbK2dREmA= \n192.168.24.19,overcloud-openshift-0.localdomain,overcloud-openshift-0,192.168.24.19,overcloud-openshift-0.external.localdomain,overcloud-openshift-0.external,192.168.24.19,overcloud-openshift-0.internalapi.localdomain,overcloud-openshift-0.internalapi,192.168.24.19,overcloud-openshift-0.storage.localdomain,overcloud-openshift-0.storage,192.168.24.19,overcloud-openshift-0.storagemgmt.localdomain,overcloud-openshift-0.storagemgmt,192.168.24.19,overcloud-openshift-0.tenant.localdomain,overcloud-openshift-0.tenant,192.168.24.19,overcloud-openshift-0.management.localdomain,overcloud-openshift-0.management,192.168.24.19,overcloud-openshift-0.ctlplane.localdomain,overcloud-openshift-0.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBKLxrTTu0EXgF9iGm68DzA1hgRCoutNYPKT575dLCzcjBkqfA0Pi+unkMhbg0bvc0aJ1Udy7DVCj6hsaOkwoLtQ= \n192.168.24.10,overcloud-openshift-1.localdomain,overcloud-openshift-1,192.168.24.10,overcloud-openshift-1.external.localdomain,overcloud-openshift-1.external,192.168.24.10,overcloud-openshift-1.internalapi.localdomain,overcloud-openshift-1.internalapi,192.168.24.10,overcloud-openshift-1.storage.localdomain,overcloud-openshift-1.storage,192.168.24.10,overcloud-openshift-1.storagemgmt.localdomain,overcloud-openshift-1.storagemgmt,192.168.24.10,overcloud-openshift-1.tenant.localdomain,overcloud-openshift-1.tenant,192.168.24.10,overcloud-openshift-1.management.localdomain,overcloud-openshift-1.management,192.168.24.10,overcloud-openshift-1.ctlplane.localdomain,overcloud-openshift-1.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBPiEB1dhR7eivaMXYFWJdEOMHzuqUl57zcigvCDejb7s/VB7uTTwaq3ejbGrRIZHeZCwIfd5V3OIa223xgX6vLM= \n192.168.24.14,overcloud-openshift-2.localdomain,overcloud-openshift-2,192.168.24.14,overcloud-openshift-2.external.localdomain,overcloud-openshift-2.external,192.168.24.14,overcloud-openshift-2.internalapi.localdomain,overcloud-openshift-2.internalapi,192.168.24.14,overcloud-openshift-2.storage.localdomain,overcloud-openshift-2.storage,192.168.24.14,overcloud-openshift-2.storagemgmt.localdomain,overcloud-openshift-2.storagemgmt,192.168.24.14,overcloud-openshift-2.tenant.localdomain,overcloud-openshift-2.tenant,192.168.24.14,overcloud-openshift-2.management.localdomain,overcloud-openshift-2.management,192.168.24.14,overcloud-openshift-2.ctlplane.localdomain,overcloud-openshift-2.ctlplane ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBHtOOAGw7PDi9xzpzzSkUQrpd/tT+MQ+FnokM5sHKwt+7rxEjhXPuiYhmZLxoVxGuVXrwFaHtFTNAUuSMXy9hC0= \n", "description": ""}, {"type": "String", "name": "deploy_server_id", "value": "aa5c6139-4a3d-4175-b35d-e0c354d8ef4b", "description": "ID of the server being deployed to"}, {"type": "String", "name": "deploy_action", "value": "CREATE", "description": "Name of the current action being deployed"}, {"type": "String", "name": "deploy_stack_id", "value": "overcloud-baremetal-ControllerSshKnownHostsDeployment-6ccwm4rlmduz/8a749c7b-da14-443c-b45e-25de8d0ea3f5", "description": "ID of the stack this deployment belongs to"}, {"type": "String", "name": "deploy_resource_name", "value": "0", "description": "Name of this deployment resource in the stack"}, {"type": "String", "name": "deploy_signal_transport", "value": "CFN_SIGNAL", "description": "How the server should signal to heat with the deployment output values."}, {"type": "String", "name": "deploy_signal_id", "value": "http://192.168.24.1:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3Ad1529b2fc9454dcf8dd0271abab7de1d%3Astacks%2Fovercloud-baremetal-ControllerSshKnownHostsDeployment-6ccwm4rlmduz%2F8a749c7b-da14-443c-b45e-25de8d0ea3f5%2Fresources%2F0?Timestamp=2017-05-17T19%3A22%3A51Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=7e84336eae6c45a3ae80b48a1ea8b911&SignatureVersion=2&Signature=NXCa%2BUESELSbSIrJpQOSt2NVJwmhWO%2BCkhlAgLuf%2ByQ%3D", "description": "ID of signal to use for signaling output values"}, {"type": "String", "name": "deploy_signal_verb", "value": "POST", "description": "HTTP verb to use for signaling outputvalues"}], "group": "script", "name": "ControllerSshKnownHostsDeployment", "outputs": [], "creation_time": "2017-05-17T19:23:02Z", "options": {}, "config": "#!/bin/bash\nset -eux\nset -o pipefail\n\necho \"Creating ssh known hosts file\"\n\nif [ ! -z \"${known_hosts}\" ]; then\n  echo \"${known_hosts}\"\n  echo -ne \"${known_hosts}\" > /etc/ssh/ssh_known_hosts\n  chmod 0644 /etc/ssh/ssh_known_hosts\nelse\n  rm -f /etc/ssh/ssh_known_hosts\n  echo \"No ssh known hosts\"\nfi\n", "id": "bdec7f93-915f-4dcc-a571-475ddb912abc"}, {"inputs": [{"type": "String", "name": "deploy_server_id", "value": "aa5c6139-4a3d-4175-b35d-e0c354d8ef4b", "description": "ID of the server being deployed to"}, {"type": "String", "name": "deploy_action", "value": "CREATE", "description": "Name of the current action being deployed"}, {"type": "String", "name": "deploy_stack_id", "value": "overcloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm/d0b4efff-786e-4da2-85cf-a2becd27242a", "description": "ID of the stack this deployment belongs to"}, {"type": "String", "name": "deploy_resource_name", "value": "ControllerUpgradeInitDeployment", "description": "Name of this deployment resource in the stack"}, {"type": "String", "name": "deploy_signal_transport", "value": "CFN_SIGNAL", "description": "How the server should signal to heat with the deployment output values."}, {"type": "String", "name": "deploy_signal_id", "value": "http://192.168.24.1:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3Ad1529b2fc9454dcf8dd0271abab7de1d%3Astacks%2Fovercloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm%2Fd0b4efff-786e-4da2-85cf-a2becd27242a%2Fresources%2FControllerUpgradeInitDeployment?Timestamp=2017-05-17T19%3A14%3A08Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=707b21092f9e4a079d9c9c00c7b5cc3f&SignatureVersion=2&Signature=1iisLmPbJozwlpLOZ1756wKOJGMrD5NvqLK3NhXoQr8%3D", "description": "ID of signal to use for signaling output values"}, {"type": "String", "name": "deploy_signal_verb", "value": "POST", "description": "HTTP verb to use for signaling outputvalues"}], "group": "script", "name": "ControllerUpgradeInitDeployment", "outputs": [], "creation_time": "2017-05-17T19:21:04Z", "options": {}, "config": "#!/bin/bash\n\nif [[ -f /etc/resolv.conf.save ]] ; then rm /etc/resolv.conf.save; fi\n\n", "id": "fd12fea1-d8d2-44c9-bab1-849029e94403"}, {"inputs": [{"type": "String", "name": "interface_name", "value": "nic1"}, {"type": "String", "name": "bridge_name", "value": "br-ex"}, {"type": "String", "name": "deploy_server_id", "value": "aa5c6139-4a3d-4175-b35d-e0c354d8ef4b", "description": "ID of the server being deployed to"}, {"type": "String", "name": "deploy_action", "value": "CREATE", "description": "Name of the current action being deployed"}, {"type": "String", "name": "deploy_stack_id", "value": "overcloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm/d0b4efff-786e-4da2-85cf-a2becd27242a", "description": "ID of the stack this deployment belongs to"}, {"type": "String", "name": "deploy_resource_name", "value": "NetworkDeployment", "description": "Name of this deployment resource in the stack"}, {"type": "String", "name": "deploy_signal_transport", "value": "CFN_SIGNAL", "description": "How the server should signal to heat with the deployment output values."}, {"type": "String", "name": "deploy_signal_id", "value": "http://192.168.24.1:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3Ad1529b2fc9454dcf8dd0271abab7de1d%3Astacks%2Fovercloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm%2Fd0b4efff-786e-4da2-85cf-a2becd27242a%2Fresources%2FNetworkDeployment?Timestamp=2017-05-17T19%3A14%3A09Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=3b73ed4c349f4c408d1c8e0b9cf7b876&SignatureVersion=2&Signature=lIuX8QEJROJtSgO9qr98y3AcgZTYr75JJZYWmTYhw3A%3D", "description": "ID of signal to use for signaling output values"}, {"type": "String", "name": "deploy_signal_verb", "value": "POST", "description": "HTTP verb to use for signaling outputvalues"}], "group": "script", "name": "NetworkDeployment", "outputs": [], "creation_time": "2017-05-17T19:20:16Z", "options": {}, "config": "#!/bin/bash\n# The following environment variables may be set to substitute in a\n# custom bridge or interface name.  Normally these are provided by the calling\n# SoftwareConfig resource, but they may also be set manually for testing.\n# $bridge_name : The bridge device name to apply\n# $interface_name : The interface name to apply\n#\n# Also this token is replaced via a str_replace in the SoftwareConfig running\n# the script - in future we may extend this to also work with a variable, e.g\n# a deployment input via input_values\n# {\"network_config\": [{\"members\": [{\"name\": \"interface_name\", \"primary\": true, \"type\": \"interface\"}], \"name\": \"bridge_name\", \"type\": \"ovs_bridge\", \"use_dhcp\": true}]} : the json serialized os-net-config config to apply\n#\nset -eux\n\nfunction get_metadata_ip() {\n\n  local METADATA_IP\n\n  # Look for a variety of Heat transports\n  # FIXME: Heat should provide a way to obtain this in a single place\n  for URL in os-collect-config.cfn.metadata_url os-collect-config.heat.auth_url os-collect-config.request.metadata_url os-collect-config.zaqar.auth_url; do\n    METADATA_IP=$(os-apply-config --key $URL --key-default '' --type raw 2>/dev/null | sed -e 's|http.*://\\([^:]*\\).*|\\1|')\n    [ -n \"$METADATA_IP\" ] && break\n  done\n\n  echo $METADATA_IP\n\n}\n\nfunction is_local_ip() {\n  local IP_TO_CHECK=$1\n  if ip -o a | grep \"inet6\\? $IP_TO_CHECK/\" &>/dev/null; then\n    return 0\n  else\n    return 1\n  fi\n}\n\nfunction ping_metadata_ip() {\n  local METADATA_IP=$(get_metadata_ip)\n\n  if [ -n \"$METADATA_IP\" ] && ! is_local_ip $METADATA_IP; then\n\n    echo -n \"Trying to ping metadata IP ${METADATA_IP}...\"\n\n    local COUNT=0\n    until ping -c 1 $METADATA_IP &> /dev/null; do\n      COUNT=$(( $COUNT + 1 ))\n      if [ $COUNT -eq 10 ]; then\n        echo \"FAILURE\"\n        echo \"$METADATA_IP is not pingable.\" >&2\n        exit 1\n      fi\n    done\n    echo \"SUCCESS\"\n\n  else\n    echo \"No metadata IP found. Skipping.\"\n  fi\n}\n\nfunction configure_safe_defaults() {\n\n[[ $? == 0 ]] && return 0\n\ncat > /etc/os-net-config/dhcp_all_interfaces.yaml <<EOF_CAT\n# This file is an autogenerated safe defaults file for os-net-config\n# which runs DHCP on all discovered interfaces to ensure connectivity\n# back to the undercloud for updates\nnetwork_config:\nEOF_CAT\n\n    for iface in $(ls /sys/class/net | grep -v ^lo$); do\n        local mac_addr_type=\"$(cat /sys/class/net/${iface}/addr_assign_type)\"\n        if [ \"$mac_addr_type\" != \"0\" ]; then\n            echo \"Device has generated MAC, skipping.\"\n        else\n            ip link set dev $iface up &>/dev/null\n            HAS_LINK=\"$(cat /sys/class/net/${iface}/carrier)\"\n\n            TRIES=10\n            while [ \"$HAS_LINK\" == \"0\" -a $TRIES -gt 0 ]; do\n                HAS_LINK=\"$(cat /sys/class/net/${iface}/carrier)\"\n                if [ \"$HAS_LINK\" == \"1\" ]; then\n                    break\n                else\n                    sleep 1\n                fi\n                TRIES=$(( TRIES - 1 ))\n            done\n            if [ \"$HAS_LINK\" == \"1\" ] ; then\ncat >> /etc/os-net-config/dhcp_all_interfaces.yaml <<EOF_CAT\n  -\n    type: interface\n    name: $iface\n    use_dhcp: true\nEOF_CAT\n            fi\n        fi\n    done\n    set +e\n    os-net-config -c /etc/os-net-config/dhcp_all_interfaces.yaml -v --detailed-exit-codes --cleanup\n    RETVAL=$?\n    set -e\n    if [[ $RETVAL == 2 ]]; then\n        ping_metadata_ip\n    elif [[ $RETVAL != 0 ]]; then\n        echo \"ERROR: configuration of safe defaults failed.\"\n    fi\n}\n\nif [ -n '{\"network_config\": [{\"members\": [{\"name\": \"interface_name\", \"primary\": true, \"type\": \"interface\"}], \"name\": \"bridge_name\", \"type\": \"ovs_bridge\", \"use_dhcp\": true}]}' ]; then\n    if [ -z \"${disable_configure_safe_defaults:-''}\" ]; then\n        trap configure_safe_defaults EXIT\n    fi\n\n    mkdir -p /etc/os-net-config\n    # Note these variables come from the calling heat SoftwareConfig\n    echo '{\"network_config\": [{\"members\": [{\"name\": \"interface_name\", \"primary\": true, \"type\": \"interface\"}], \"name\": \"bridge_name\", \"type\": \"ovs_bridge\", \"use_dhcp\": true}]}' > /etc/os-net-config/config.json\n\n    if [ \"$(type -t network_config_hook)\" = \"function\" ]; then\n        network_config_hook\n    fi\n\n    sed -i \"s/bridge_name/${bridge_name:-''}/\" /etc/os-net-config/config.json\n    sed -i \"s/interface_name/${interface_name:-''}/\" /etc/os-net-config/config.json\n\n    set +e\n    os-net-config -c /etc/os-net-config/config.json -v --detailed-exit-codes\n    RETVAL=$?\n    set -e\n\n    if [[ $RETVAL == 2 ]]; then\n        ping_metadata_ip\n\n        #NOTE: dprince this udev rule can apparently leak DHCP processes?\n        # https://bugs.launchpad.net/tripleo/+bug/1538259\n        # until we discover the root cause we can simply disable the\n        # rule because networking has already been configured at this point\n        if [ -f /etc/udev/rules.d/99-dhcp-all-interfaces.rules ]; then\n            rm /etc/udev/rules.d/99-dhcp-all-interfaces.rules\n        fi\n\n    elif [[ $RETVAL != 0 ]]; then\n        echo \"ERROR: os-net-config configuration failed.\" >&2\n        exit 1\n    fi\nfi\n", "id": "9d2a1446-2061-482a-bd1e-d94234aacd64"}, {"inputs": [{"default": "", "type": "String", "name": "update_identifier", "value": "", "description": "yum will only run for previously unused values of update_identifier"}, {"default": "update", "type": "String", "name": "command", "value": "update", "description": "yum sub-command to run, defaults to \"update\""}, {"default": "", "type": "String", "name": "command_arguments", "value": "", "description": "yum command arguments, defaults to \"\""}, {"type": "String", "name": "deploy_server_id", "value": "aa5c6139-4a3d-4175-b35d-e0c354d8ef4b", "description": "ID of the server being deployed to"}, {"type": "String", "name": "deploy_action", "value": "CREATE", "description": "Name of the current action being deployed"}, {"type": "String", "name": "deploy_stack_id", "value": "overcloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm/d0b4efff-786e-4da2-85cf-a2becd27242a", "description": "ID of the stack this deployment belongs to"}, {"type": "String", "name": "deploy_resource_name", "value": "UpdateDeployment", "description": "Name of this deployment resource in the stack"}, {"type": "String", "name": "deploy_signal_transport", "value": "CFN_SIGNAL", "description": "How the server should signal to heat with the deployment output values."}, {"type": "String", "name": "deploy_signal_id", "value": "http://192.168.24.1:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3Ad1529b2fc9454dcf8dd0271abab7de1d%3Astacks%2Fovercloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm%2Fd0b4efff-786e-4da2-85cf-a2becd27242a%2Fresources%2FUpdateDeployment?Timestamp=2017-05-17T19%3A14%3A08Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=88e81672c016485b858abc4336c9e335&SignatureVersion=2&Signature=r9oX8vlcjReEigga56EpODuT9%2FxHzCxakVBz%2BCZ9iSc%3D", "description": "ID of signal to use for signaling output values"}, {"type": "String", "name": "deploy_signal_verb", "value": "POST", "description": "HTTP verb to use for signaling outputvalues"}], "group": "script", "name": "UpdateDeployment", "outputs": [{"type": "String", "name": "update_managed_packages", "error_output": false, "description": "boolean value indicating whether to upgrade managed packages"}], "creation_time": "2017-05-17T19:20:59Z", "options": {}, "config": "#!/bin/bash\n\nset -eu\n\nDEBUG=\"true\" # set false if the verbosity is a problem\nSCRIPT_NAME=$(basename $0)\nfunction log_debug {\n  if [[ $DEBUG = \"true\" ]]; then\n    echo \"`date` $SCRIPT_NAME tripleo-upgrade $(facter hostname) $1\"\n  fi\n}\n\nfunction is_bootstrap_node {\n  if [ \"$(hiera -c /etc/puppet/hiera.yaml bootstrap_nodeid)\" = \"$(facter hostname)\" ]; then\n    log_debug \"Node is bootstrap\"\n    echo \"true\"\n  fi\n}\n\nfunction check_resource_pacemaker {\n  if [ \"$#\" -ne 3 ]; then\n    echo_error \"ERROR: check_resource function expects 3 parameters, $# given\"\n    exit 1\n  fi\n\n  local service=$1\n  local state=$2\n  local timeout=$3\n\n  if [[ -z $(is_bootstrap_node) ]] ; then\n    log_debug \"Node isn't bootstrap, skipping check for $service to be $state here \"\n    return\n  else\n    log_debug \"Node is bootstrap checking $service to be $state here\"\n  fi\n\n  if [ \"$state\" = \"stopped\" ]; then\n    match_for_incomplete='Started'\n  else # started\n    match_for_incomplete='Stopped'\n  fi\n\n  nodes_local=$(pcs status  | grep ^Online | sed 's/.*\\[ \\(.*\\) \\]/\\1/g' | sed 's/ /\\|/g')\n  if timeout -k 10 $timeout crm_resource --wait; then\n    node_states=$(pcs status --full | grep \"$service\" | grep -v Clone | { egrep \"$nodes_local\" || true; } )\n    if echo \"$node_states\" | grep -q \"$match_for_incomplete\"; then\n      echo_error \"ERROR: cluster finished transition but $service was not in $state state, exiting.\"\n      exit 1\n    else\n      echo \"$service has $state\"\n    fi\n  else\n    echo_error \"ERROR: cluster remained unstable for more than $timeout seconds, exiting.\"\n    exit 1\n  fi\n\n}\n\nfunction pcmk_running {\n  if [[ $(systemctl is-active pacemaker) = \"active\" ]] ; then\n    echo \"true\"\n  fi\n}\n\nfunction is_systemd_unknown {\n  local service=$1\n  if [[ $(systemctl is-active \"$service\") = \"unknown\" ]]; then\n    log_debug \"$service found to be unkown to systemd\"\n    echo \"true\"\n  fi\n}\n\nfunction grep_is_cluster_controlled {\n  local service=$1\n  if [[ -n $(systemctl status $service -l | grep Drop-In -A 5 | grep pacemaker) ||\n      -n $(systemctl status $service -l | grep \"Cluster Controlled $service\") ]] ; then\n    log_debug \"$service is pcmk managed from systemctl grep\"\n    echo \"true\"\n  fi\n}\n\n\nfunction is_systemd_managed {\n  local service=$1\n  #if we have pcmk check to see if it is managed there\n  if [[ -n $(pcmk_running) ]]; then\n    if [[ -z $(pcs status --full | grep $service)  && -z $(is_systemd_unknown $service) ]] ; then\n      log_debug \"$service found to be systemd managed from pcs status\"\n      echo \"true\"\n    fi\n  else\n    # if it is \"unknown\" to systemd, then it is pacemaker managed\n    if [[  -n $(is_systemd_unknown $service) ]] ; then\n      return\n    elif [[ -z $(grep_is_cluster_controlled $service) ]] ; then\n      echo \"true\"\n    fi\n  fi\n}\n\nfunction is_pacemaker_managed {\n  local service=$1\n  #if we have pcmk check to see if it is managed there\n  if [[ -n $(pcmk_running) ]]; then\n    if [[ -n $(pcs status --full | grep $service) ]]; then\n      log_debug \"$service found to be pcmk managed from pcs status\"\n      echo \"true\"\n    fi\n  else\n    # if it is unknown to systemd, then it is pcmk managed\n    if [[ -n $(is_systemd_unknown $service) ]]; then\n      echo \"true\"\n    elif [[ -n $(grep_is_cluster_controlled $service) ]] ; then\n      echo \"true\"\n    fi\n  fi\n}\n\nfunction is_managed {\n  local service=$1\n  if [[ -n $(is_pacemaker_managed $service) || -n $(is_systemd_managed $service) ]]; then\n    echo \"true\"\n  fi\n}\n\nfunction check_resource_systemd {\n\n  if [ \"$#\" -ne 3 ]; then\n    echo_error \"ERROR: check_resource function expects 3 parameters, $# given\"\n    exit 1\n  fi\n\n  local service=$1\n  local state=$2\n  local timeout=$3\n  local check_interval=3\n\n  if [ \"$state\" = \"stopped\" ]; then\n    match_for_incomplete='active'\n  else # started\n    match_for_incomplete='inactive'\n  fi\n\n  log_debug \"Going to check_resource_systemd for $service to be $state\"\n\n  #sanity check is systemd managed:\n  if [[ -z $(is_systemd_managed $service) ]]; then\n    echo \"ERROR - $service not found to be systemd managed.\"\n    exit 1\n  fi\n\n  tstart=$(date +%s)\n  tend=$(( $tstart + $timeout ))\n  while (( $(date +%s) < $tend )); do\n    if [[ \"$(systemctl is-active $service)\" = $match_for_incomplete ]]; then\n      echo \"$service not yet $state, sleeping $check_interval seconds.\"\n      sleep $check_interval\n    else\n      echo \"$service is $state\"\n      return\n    fi\n  done\n\n  echo \"Timed out waiting for $service to go to $state after $timeout seconds\"\n  exit 1\n}\n\n\nfunction check_resource {\n  local service=$1\n  local pcmk_managed=$(is_pacemaker_managed $service)\n  local systemd_managed=$(is_systemd_managed $service)\n\n  if [[ -n $pcmk_managed && -n $systemd_managed ]] ; then\n    log_debug \"ERROR $service managed by both systemd and pcmk - SKIPPING\"\n    return\n  fi\n\n  if [[ -n $pcmk_managed ]]; then\n    check_resource_pacemaker $@\n    return\n  elif [[ -n $systemd_managed ]]; then\n    check_resource_systemd $@\n    return\n  fi\n  log_debug \"ERROR cannot check_resource for $service, not managed here?\"\n}\n\nfunction manage_systemd_service {\n  local action=$1\n  local service=$2\n  log_debug \"Going to systemctl $action $service\"\n  systemctl $action $service\n}\n\nfunction manage_pacemaker_service {\n  local action=$1\n  local service=$2\n  # not if pacemaker isn't running!\n  if [[ -z $(pcmk_running) ]]; then\n    echo \"$(facter hostname) pacemaker not active, skipping $action $service here\"\n  elif [[ -n $(is_bootstrap_node) ]]; then\n    log_debug \"Going to pcs resource $action $service\"\n    pcs resource $action $service\n  fi\n}\n\nfunction stop_or_disable_service {\n  local service=$1\n  local pcmk_managed=$(is_pacemaker_managed $service)\n  local systemd_managed=$(is_systemd_managed $service)\n\n  if [[ -n $pcmk_managed && -n $systemd_managed ]] ; then\n    log_debug \"Skipping stop_or_disable $service due to management conflict\"\n    return\n  fi\n\n  log_debug \"Stopping or disabling $service\"\n  if [[ -n $pcmk_managed ]]; then\n    manage_pacemaker_service disable $service\n    return\n  elif [[ -n $systemd_managed ]]; then\n    manage_systemd_service stop $service\n    return\n  fi\n  log_debug \"ERROR: $service not managed here?\"\n}\n\nfunction start_or_enable_service {\n  local service=$1\n  local pcmk_managed=$(is_pacemaker_managed $service)\n  local systemd_managed=$(is_systemd_managed $service)\n\n  if [[ -n $pcmk_managed && -n $systemd_managed ]] ; then\n    log_debug \"Skipping start_or_enable $service due to management conflict\"\n    return\n  fi\n\n  log_debug \"Starting or enabling $service\"\n  if [[ -n $pcmk_managed ]]; then\n    manage_pacemaker_service enable $service\n    return\n  elif [[ -n $systemd_managed ]]; then\n    manage_systemd_service start $service\n    return\n  fi\n  log_debug \"ERROR $service not managed here?\"\n}\n\nfunction restart_service {\n  local service=$1\n  local pcmk_managed=$(is_pacemaker_managed $service)\n  local systemd_managed=$(is_systemd_managed $service)\n\n  if [[ -n $pcmk_managed && -n $systemd_managed ]] ; then\n    log_debug \"ERROR $service managed by both systemd and pcmk - SKIPPING\"\n    return\n  fi\n\n  log_debug \"Restarting $service\"\n  if [[ -n $pcmk_managed ]]; then\n    manage_pacemaker_service restart $service\n    return\n  elif [[ -n $systemd_managed ]]; then\n    manage_systemd_service restart $service\n    return\n  fi\n  log_debug \"ERROR $service not managed here?\"\n}\n\nfunction echo_error {\n    echo \"$@\" | tee /dev/fd2\n}\n\n# swift is a special case because it is/was never handled by pacemaker\n# when stand-alone swift is used, only swift-proxy is running on controllers\nfunction systemctl_swift {\n    services=( openstack-swift-account-auditor openstack-swift-account-reaper openstack-swift-account-replicator openstack-swift-account \\\n               openstack-swift-container-auditor openstack-swift-container-replicator openstack-swift-container-updater openstack-swift-container \\\n               openstack-swift-object-auditor openstack-swift-object-replicator openstack-swift-object-updater openstack-swift-object openstack-swift-proxy )\n    local action=$1\n    case $action in\n        stop)\n            services=$(systemctl | grep openstack-swift- | grep running | awk '{print $1}')\n            ;;\n        start)\n            enable_swift_storage=$(hiera -c /etc/puppet/hiera.yaml tripleo::profile::base::swift::storage::enable_swift_storage)\n            if [[ $enable_swift_storage != \"true\" ]]; then\n                services=( openstack-swift-proxy )\n            fi\n            ;;\n        *)  echo \"Unknown action $action passed to systemctl_swift\"\n            exit 1\n            ;; # shouldn't ever happen...\n    esac\n    for service in ${services[@]}; do\n        manage_systemd_service $action $service\n    done\n}\n\n# Special-case OVS for https://bugs.launchpad.net/tripleo/+bug/1635205\n# Update condition and add --notriggerun for +bug/1669714\nfunction special_case_ovs_upgrade_if_needed {\n    if rpm -qa | grep \"^openvswitch-2.5.0-14\" || rpm -q --scripts openvswitch | awk '/postuninstall/,/*/' | grep \"systemctl.*try-restart\" ; then\n        echo \"Manual upgrade of openvswitch - ovs-2.5.0-14 or restart in postun detected\"\n        rm -rf OVS_UPGRADE\n        mkdir OVS_UPGRADE && pushd OVS_UPGRADE\n        echo \"Attempting to downloading latest openvswitch with yumdownloader\"\n        yumdownloader --resolve openvswitch\n        for pkg in $(ls -1 *.rpm);  do\n            if rpm -U --test $pkg 2>&1 | grep \"already installed\" ; then\n                echo \"Looks like newer version of $pkg is already installed, skipping\"\n            else\n                echo \"Updating $pkg with --nopostun --notriggerun\"\n                rpm -U --replacepkgs --nopostun --notriggerun $pkg\n            fi\n        done\n        popd\n    else\n        echo \"Skipping manual upgrade of openvswitch - no restart in postun detected\"\n    fi\n\n}\n\n# This code is meant to fix https://bugs.launchpad.net/tripleo/+bug/1686357 on\n# existing setups via a minor update workflow and be idempotent. We need to\n# run this before the yum update because we fix this up even when there are no\n# packages to update on the system (in which case the script exits).\n# This code must be called with set +eu (due to the ocf scripts being sourced)\nfunction fixup_wrong_ipv6_vip {\n    # This XPath query identifies of all the VIPs in pacemaker with netmask /64. Those are IPv6 only resources that have the wrong netmask\n    # This gives the address of the resource in the CIB, one address per line. For example:\n    # /cib/configuration/resources/primitive[@id='ip-2001.db8.ca2.4..10']/instance_attributes[@id='ip-2001.db8.ca2.4..10-instance_attributes']\\\n    # /nvpair[@id='ip-2001.db8.ca2.4..10-instance_attributes-cidr_netmask']\n    vip_xpath_query=\"//resources/primitive[@type='IPaddr2']/instance_attributes/nvpair[@name='cidr_netmask' and @value='64']\"\n    vip_xpath_xml_addresses=$(cibadmin --query --xpath \"$vip_xpath_query\" -e 2>/dev/null)\n    # The following extracts the @id value of the resource\n    vip_resources_to_fix=$(echo -e \"$vip_xpath_xml_addresses\" | sed -n \"s/.*primitive\\[@id='\\([^']*\\)'.*/\\1/p\")\n    # Runnning this in a subshell so that sourcing files cannot possibly affect the running script\n    (\n        OCF_PATH=\"/usr/lib/ocf/lib/heartbeat\"\n        if [ -n \"$vip_resources_to_fix\" -a -f $OCF_PATH/ocf-shellfuncs -a -f $OCF_PATH/findif.sh ]; then\n            source $OCF_PATH/ocf-shellfuncs\n            source $OCF_PATH/findif.sh\n            for resource in $vip_resources_to_fix; do\n                echo \"Updating IPv6 VIP $resource with a /128 and a correct addrlabel\"\n                # The following will give us something like:\n                # <nvpair id=\"ip-2001.db8.ca2.4..10-instance_attributes-ip\" name=\"ip\" value=\"2001:db8:ca2:4::10\"/>\n                ip_cib_nvpair=$(cibadmin --query --xpath \"//resources/primitive[@type='IPaddr2' and @id='$resource']/instance_attributes/nvpair[@name='ip']\")\n                # Let's filter out the value of the nvpair to get the ip address\n                ip_address=$(echo $ip_cib_nvpair | xmllint --xpath 'string(//nvpair/@value)' -)\n                OCF_RESKEY_cidr_netmask=\"64\"\n                OCF_RESKEY_ip=\"$ip_address\"\n                # Unfortunately due to https://bugzilla.redhat.com/show_bug.cgi?id=1445628\n                # we need to find out the appropiate nic given the ip address.\n                nic=$(findif $ip_address | awk '{ print $1 }')\n                ret=$?\n                if [ -z \"$nic\" -o $ret -ne 0 ]; then\n                    echo \"NIC autodetection failed for VIP $ip_address, not updating VIPs\"\n                    # Only exits the subshell\n                    exit 1\n                fi\n                ocf_run -info pcs resource update --wait \"$resource\" ip=\"$ip_address\" cidr_netmask=128 nic=\"$nic\" lvs_ipv6_addrlabel=true lvs_ipv6_addrlabel_value=99\n                ret=$?\n                if [ $ret -ne 0 ]; then\n                    echo \"pcs resource update for VIP $resource failed, not updating VIPs\"\n                    # Only exits the subshell\n                    exit 1\n                fi\n            done\n        fi\n    )\n}\n#!/bin/bash\n\n# A heat-config-script which runs yum update during a stack-update.\n# Inputs:\n#   deploy_action - yum will only be run if this is UPDATE\n#   update_identifier - yum will only run for previously unused values of update_identifier\n#   command - yum sub-command to run, defaults to \"update\"\n#   command_arguments - yum command arguments, defaults to \"\"\n\necho \"Started yum_update.sh on server $deploy_server_id at `date`\"\necho -n \"false\" > $heat_outputs_path.update_managed_packages\n\nif [ -f /.dockerenv ]; then\n    echo \"Not running due to running inside a container\"\n    exit 0\nfi\n\nif [[ -z \"$update_identifier\" ]]; then\n    echo \"Not running due to unset update_identifier\"\n    exit 0\nfi\n\ntimestamp_dir=/var/lib/overcloud-yum-update\nmkdir -p $timestamp_dir\n\n# sanitise to remove unusual characters\nupdate_identifier=${update_identifier//[^a-zA-Z0-9-_]/}\n\n# seconds to wait for this node to rejoin the cluster after update\ncluster_start_timeout=600\ngalera_sync_timeout=1800\ncluster_settle_timeout=1800\n\ntimestamp_file=\"$timestamp_dir/$update_identifier\"\nif [[ -a \"$timestamp_file\" ]]; then\n    echo \"Not running for already-run timestamp \\\"$update_identifier\\\"\"\n    exit 0\nfi\ntouch \"$timestamp_file\"\n\npacemaker_status=\"\"\n# We include word boundaries in order to not match pacemaker_remote\nif hiera -c /etc/puppet/hiera.yaml service_names | grep -q '\\bpacemaker\\b'; then\n    pacemaker_status=$(systemctl is-active pacemaker)\nfi\n\n# (NB: when backporting this s/pacemaker_short_bootstrap_node_name/bootstrap_nodeid)\n# This runs before the yum_update so we are guaranteed to run it even in the absence\n# of packages to update (the check for -z \"$update_identifier\" guarantees that this\n# is run only on overcloud stack update -i)\nif [[ \"$pacemaker_status\" == \"active\" && \\\n        \"$(hiera -c /etc/puppet/hiera.yaml pacemaker_short_bootstrap_node_name)\" == \"$(facter hostname)\" ]] ; then \\\n    # OCF scripts don't cope with -eu\n    echo \"Verifying if we need to fix up any IPv6 VIPs\"\n    set +eu\n    fixup_wrong_ipv6_vip\n    ret=$?\n    set -eu\n    if [ $ret -ne 0 ]; then\n        echo \"Fixing up IPv6 VIPs failed. Stopping here. (See https://bugs.launchpad.net/tripleo/+bug/1686357 for more info)\"\n        exit 1\n    fi\nfi\n\ncommand_arguments=${command_arguments:-}\n\n# yum check-update exits 100 if updates are available\nset +e\ncheck_update=$(yum check-update 2>&1)\ncheck_update_exit=$?\nset -e\n\nif [[ \"$check_update_exit\" == \"1\" ]]; then\n    echo \"Failed to check for package updates\"\n    echo \"$check_update\"\n    exit 1\nelif [[ \"$check_update_exit\" != \"100\" ]]; then\n    echo \"No packages require updating\"\n    exit 0\nfi\n\n# special case https://bugs.launchpad.net/tripleo/+bug/1635205 +bug/1669714\nspecial_case_ovs_upgrade_if_needed\n\nif [[ \"$pacemaker_status\" == \"active\" ]] ; then\n    echo \"Pacemaker running, stopping cluster node and doing full package update\"\n    node_count=$(pcs status xml | grep -o \"<nodes_configured.*/>\" | grep -o 'number=\"[0-9]*\"' | grep -o \"[0-9]*\")\n    if [[ \"$node_count\" == \"1\" ]] ; then\n        echo \"Active node count is 1, stopping node with --force\"\n        pcs cluster stop --force\n    else\n        pcs cluster stop\n    fi\nelse\n    echo \"Upgrading openstack-puppet-modules and its dependencies\"\n    yum -q -y update openstack-puppet-modules\n    yum deplist openstack-puppet-modules | awk '/dependency/{print $2}' | xargs yum -q -y update\n    echo \"Upgrading other packages is handled by config management tooling\"\n    echo -n \"true\" > $heat_outputs_path.update_managed_packages\n    exit 0\nfi\n\ncommand=${command:-update}\nfull_command=\"yum -q -y $command $command_arguments\"\necho \"Running: $full_command\"\n\nresult=$($full_command)\nreturn_code=$?\necho \"$result\"\necho \"yum return code: $return_code\"\n\nif [[ \"$pacemaker_status\" == \"active\" ]] ; then\n    echo \"Starting cluster node\"\n    pcs cluster start\n\n    hostname=$(hostname -s)\n    tstart=$(date +%s)\n    while [[ \"$(pcs status | grep \"^Online\" | grep -F -o $hostname)\" == \"\" ]]; do\n        sleep 5\n        tnow=$(date +%s)\n        if (( tnow-tstart > cluster_start_timeout )) ; then\n            echo \"ERROR $hostname failed to join cluster in $cluster_start_timeout seconds\"\n            pcs status\n            exit 1\n        fi\n    done\n\n    RETVAL=$( pcs resource show galera-master | grep wsrep_cluster_address | grep -q `crm_node -n` ; echo $? )\n\n    if [[ $RETVAL -eq 0 && -e /etc/sysconfig/clustercheck ]]; then\n        tstart=$(date +%s)\n        while ! clustercheck; do\n            sleep 5\n            tnow=$(date +%s)\n            if (( tnow-tstart > galera_sync_timeout )) ; then\n                echo \"ERROR galera sync timed out\"\n                exit 1\n            fi\n        done\n    fi\n\n    echo \"Waiting for pacemaker cluster to settle\"\n    if ! timeout -k 10 $cluster_settle_timeout crm_resource --wait; then\n        echo \"ERROR timed out while waiting for the cluster to settle\"\n        exit 1\n    fi\n\n    pcs status\nfi\n\n\necho \"Finished yum_update.sh on server $deploy_server_id at `date` with return code: $return_code\"\n\nexit $return_code\n", "id": "595948a6-e29c-4c6b-a935-a0e4a2df74a8"}, {"inputs": [{"type": "String", "name": "deploy_server_id", "value": "aa5c6139-4a3d-4175-b35d-e0c354d8ef4b", "description": "ID of the server being deployed to"}, {"type": "String", "name": "deploy_action", "value": "CREATE", "description": "Name of the current action being deployed"}, {"type": "String", "name": "deploy_stack_id", "value": "overcloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm-SshHostPubKey-cbutfmatubdj/3cc5b05c-8dad-4c4d-a8ad-9001c88388af", "description": "ID of the stack this deployment belongs to"}, {"type": "String", "name": "deploy_resource_name", "value": "SshHostPubKeyDeployment", "description": "Name of this deployment resource in the stack"}, {"type": "String", "name": "deploy_signal_transport", "value": "CFN_SIGNAL", "description": "How the server should signal to heat with the deployment output values."}, {"type": "String", "name": "deploy_signal_id", "value": "http://192.168.24.1:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3Ad1529b2fc9454dcf8dd0271abab7de1d%3Astacks%2Fovercloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm-SshHostPubKey-cbutfmatubdj%2F3cc5b05c-8dad-4c4d-a8ad-9001c88388af%2Fresources%2FSshHostPubKeyDeployment?Timestamp=2017-05-17T19%3A21%3A50Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=92a95840152f44ab8197530a17a26c27&SignatureVersion=2&Signature=hI9b1o%2B3mCqitXFKzzR62JgQjYXofzAP83Jm46MnO4o%3D", "description": "ID of signal to use for signaling output values"}, {"type": "String", "name": "deploy_signal_verb", "value": "POST", "description": "HTTP verb to use for signaling outputvalues"}], "group": "script", "name": "overcloud-baremetal-Controller-m5qdr4cwgthi-0-qheur6dgdopm-SshHostPubKey-cbutfmatubdj-SshHostPubKeyConfig-aqs2ie7kxxpk", "outputs": [{"type": "String", "name": "rsa", "error_output": false, "description": ""}, {"type": "String", "name": "ecdsa", "error_output": false, "description": ""}, {"type": "String", "name": "ed25519", "error_output": false, "description": ""}], "creation_time": "2017-05-17T19:21:51Z", "options": {}, "config": "#!/bin/sh -x\ntest -e '/etc/ssh/ssh_host_rsa_key.pub' && cat /etc/ssh/ssh_host_rsa_key.pub > $heat_outputs_path.rsa\ntest -e '/etc/ssh/ssh_host_ecdsa_key.pub' && cat /etc/ssh/ssh_host_ecdsa_key.pub > $heat_outputs_path.ecdsa\ntest -e '/etc/ssh/ssh_host_ed25519_key.pub' && cat /etc/ssh/ssh_host_ed25519_key.pub > $heat_outputs_path.ed25519\n", "id": "174e0a86-f331-46a5-a03f-d856a0910ae4"}], "os-collect-config": {"request": {"metadata_url": "http://192.168.24.1:8080/v1/AUTH_d1529b2fc9454dcf8dd0271abab7de1d/ov-5qdr4cwgthi-0-qheur6dgdopm-Controller-w6cudim3vux3/338454fd-9ab5-4e43-91c2-f5e0089810b0?temp_url_sig=e61c63f398d47177e8be797235e7a93fc6c70fad&temp_url_expires=2147483586"}, "command": "os-refresh-config --timeout 14400", "collectors": ["ec2", "request", "local"]}}